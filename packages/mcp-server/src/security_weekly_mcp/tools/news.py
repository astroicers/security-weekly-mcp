"""æ–°èæ”¶é›† MCP å·¥å…·"""

import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

import feedparser
import httpx
from mcp.types import TextContent, Tool

# é…ç½®æª”æ¡ˆè·¯å¾‘
CONFIG_DIR = Path(__file__).parent.parent.parent.parent.parent.parent / "config"


async def list_tools() -> list[Tool]:
    """åˆ—å‡ºæ–°èæ”¶é›†ç›¸é—œå·¥å…·"""
    return [
        Tool(
            name="fetch_security_news",
            description="å¾ RSS ä¾†æºæ”¶é›†è³‡å®‰æ–°è",
            inputSchema={
                "type": "object",
                "properties": {
                    "sources": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ä¾†æºåç¨±åˆ—è¡¨ï¼ˆå¦‚ thehackernews, bleepingcomputer, ithomeï¼‰ã€‚ç•™ç©ºå‰‡ä½¿ç”¨æ‰€æœ‰ä¾†æºã€‚"
                    },
                    "days": {
                        "type": "integer",
                        "description": "å›é¡§å¤©æ•¸",
                        "default": 7
                    },
                    "keywords": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "é—œéµå­—éæ¿¾ï¼ˆç¬¦åˆä»»ä¸€å³å¯ï¼‰"
                    },
                    "limit": {
                        "type": "integer",
                        "description": "æ¯å€‹ä¾†æºçš„æœ€å¤§æ–‡ç« æ•¸",
                        "default": 10
                    }
                }
            }
        ),
        Tool(
            name="fetch_vulnerabilities",
            description="æ”¶é›†è¿‘æœŸé«˜é¢¨éšªæ¼æ´ï¼ˆNVD + CISA KEVï¼‰",
            inputSchema={
                "type": "object",
                "properties": {
                    "min_cvss": {
                        "type": "number",
                        "description": "æœ€ä½ CVSS åˆ†æ•¸",
                        "default": 7.0
                    },
                    "days": {
                        "type": "integer",
                        "description": "å›é¡§å¤©æ•¸",
                        "default": 7
                    },
                    "include_kev": {
                        "type": "boolean",
                        "description": "æ˜¯å¦åŒ…å« CISA KEVï¼ˆå·²çŸ¥è¢«åˆ©ç”¨æ¼æ´ï¼‰",
                        "default": True
                    },
                    "limit": {
                        "type": "integer",
                        "description": "æœ€å¤§å›å‚³æ•¸é‡",
                        "default": 20
                    }
                }
            }
        ),
        Tool(
            name="list_news_sources",
            description="åˆ—å‡ºå¯ç”¨çš„æ–°èä¾†æº",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        Tool(
            name="suggest_searches",
            description="ç”¢ç”Ÿ WebSearch/WebFetch æœå°‹å»ºè­°ï¼Œç”¨æ–¼è£œå…… RSS ç„¡æ³•å–å¾—çš„è³‡å®‰æ–°èã€‚æ”¯æ´æ­·å²æ™‚é–“ç¯„åœæœå°‹ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "category": {
                        "type": "string",
                        "enum": ["taiwan_news", "vulnerabilities", "threat_intel", "industry_specific", "all"],
                        "description": "æœå°‹é¡åˆ¥",
                        "default": "all"
                    },
                    "period_start": {
                        "type": "string",
                        "description": "æœå°‹èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)ï¼Œç”¨æ–¼ç”¢ç”Ÿæ­·å²é€±å ±",
                    },
                    "period_end": {
                        "type": "string",
                        "description": "æœå°‹çµæŸæ—¥æœŸ (YYYY-MM-DD)ï¼Œç”¨æ–¼ç”¢ç”Ÿæ­·å²é€±å ±",
                    },
                    "context": {
                        "type": "object",
                        "description": "å‹•æ…‹å…§å®¹ï¼ˆå¦‚ cve_id, ransomware_name, apt_groupï¼‰",
                        "additionalProperties": {"type": "string"}
                    },
                    "include_fetch_targets": {
                        "type": "boolean",
                        "description": "æ˜¯å¦åŒ…å« WebFetch ç›®æ¨™ç¶²å€",
                        "default": True
                    }
                }
            }
        ),
        Tool(
            name="load_weekly_data",
            description="è¼‰å…¥å·²ä¿å­˜çš„é€±å ±åŸå§‹è³‡æ–™ï¼ˆç”± GitHub Actions æ¯é€±è‡ªå‹•æ”¶é›†ï¼‰",
            inputSchema={
                "type": "object",
                "properties": {
                    "week": {
                        "type": "string",
                        "description": "é€±æ•¸ï¼ˆæ ¼å¼ï¼šYYYY-WNNï¼Œå¦‚ 2026-W07ï¼‰ã€‚ç•™ç©ºå‰‡è¼‰å…¥æœ€æ–°ä¸€é€±ã€‚"
                    }
                }
            }
        ),
        Tool(
            name="list_weekly_data",
            description="åˆ—å‡ºæ‰€æœ‰å·²ä¿å­˜çš„é€±å ±åŸå§‹è³‡æ–™",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
    ]


# è¨­å®šæª”å¿«å–
_sources_cache = None
_templates_cache = None


def _load_sources_config() -> dict:
    """è¼‰å…¥ä¾†æºè¨­å®šï¼ˆå¿«å–ï¼‰"""
    global _sources_cache
    if _sources_cache is None:
        import yaml
        sources_file = CONFIG_DIR / "sources.yaml"
        if sources_file.exists():
            _sources_cache = yaml.safe_load(sources_file.read_text(encoding="utf-8"))
        else:
            _sources_cache = {"sources": []}
    return _sources_cache


def _load_search_templates() -> dict:
    """è¼‰å…¥æœå°‹æ¨¡æ¿è¨­å®šï¼ˆå¿«å–ï¼‰"""
    global _templates_cache
    if _templates_cache is None:
        import yaml
        templates_file = CONFIG_DIR / "search_templates.yaml"
        if templates_file.exists():
            _templates_cache = yaml.safe_load(templates_file.read_text(encoding="utf-8"))
        else:
            _templates_cache = {}
    return _templates_cache


def reset_config_cache():
    """é‡è¨­è¨­å®šæª”å¿«å–ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰"""
    global _sources_cache, _templates_cache
    _sources_cache = None
    _templates_cache = None


def _normalize_source_name(name: str) -> str:
    """æ¨™æº–åŒ–ä¾†æºåç¨±ä»¥ä¾¿æ¯”å°"""
    return name.lower().replace(" ", "").replace("_", "").replace("-", "")


def _match_source(query: str, sources: list[dict]) -> list[dict]:
    """æ ¹æ“šæŸ¥è©¢å­—ä¸²æ¯”å°ä¾†æº"""
    query_normalized = _normalize_source_name(query)
    matched = []
    for source in sources:
        source_normalized = _normalize_source_name(source["name"])
        if query_normalized in source_normalized or source_normalized in query_normalized:
            matched.append(source)
    return matched


async def _fetch_rss(url: str, days: int, limit: int, keywords: list[str] | None = None) -> list[dict]:
    """å¾ RSS ä¾†æºæŠ“å–æ–‡ç« """
    # è¨­å®š User-Agent ä»¥é¿å…è¢«æŸäº›ç¶²ç«™å°é– (å¦‚ BleepingComputer)
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "application/rss+xml, application/xml, text/xml, */*",
    }
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(url, headers=headers, follow_redirects=True)
            response.raise_for_status()
            feed = feedparser.parse(response.text)
    except httpx.TimeoutException:
        return [{"error": "RSS æŠ“å–è¶…æ™‚ (30s)"}]
    except httpx.HTTPStatusError as e:
        return [{"error": f"HTTP {e.response.status_code}: {e.response.reason_phrase}"}]
    except httpx.RequestError as e:
        return [{"error": f"ç¶²è·¯è«‹æ±‚å¤±æ•—: {type(e).__name__}"}]
    except Exception as e:
        return [{"error": f"ç„¡æ³•æŠ“å– RSS: {e}"}]

    cutoff_date = datetime.now() - timedelta(days=days)
    articles = []

    for entry in feed.entries[:limit * 2]:  # æŠ“å¤šä¸€é»å†éæ¿¾
        # è§£æç™¼å¸ƒæ™‚é–“
        published = None
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            published = datetime(*entry.published_parsed[:6])
        elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
            published = datetime(*entry.updated_parsed[:6])

        # æ™‚é–“éæ¿¾
        if published and published < cutoff_date:
            continue

        # é—œéµå­—éæ¿¾
        if keywords:
            title = entry.get("title", "").lower()
            summary = entry.get("summary", "").lower()
            content = title + " " + summary
            if not any(kw.lower() in content for kw in keywords):
                continue

        articles.append({
            "title": entry.get("title", ""),
            "link": entry.get("link", ""),
            "published": published.isoformat() if published else None,
            "summary": entry.get("summary", "")[:500]  # æ‘˜è¦æˆªæ–·
        })

        if len(articles) >= limit:
            break

    return articles


async def _fetch_nvd(min_cvss: float, days: int, limit: int) -> list[dict]:
    """å¾ NVD æŠ“å–æ¼æ´è³‡æ–™"""
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)

    params = {
        "pubStartDate": start_date.strftime("%Y-%m-%dT00:00:00.000"),
        "pubEndDate": end_date.strftime("%Y-%m-%dT23:59:59.999"),
        "cvssV3Severity": "HIGH" if min_cvss >= 7.0 else "MEDIUM",
        "resultsPerPage": min(limit, 50)
    }

    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.get(
                "https://services.nvd.nist.gov/rest/json/cves/2.0",
                params=params,
                headers={"Accept": "application/json"}
            )
            response.raise_for_status()
            data = response.json()
    except httpx.TimeoutException:
        return [{"error": "NVD API è¶…æ™‚ (60s)"}]
    except httpx.HTTPStatusError as e:
        return [{"error": f"NVD API HTTP {e.response.status_code}"}]
    except httpx.RequestError as e:
        return [{"error": f"NVD API ç¶²è·¯éŒ¯èª¤: {type(e).__name__}"}]
    except json.JSONDecodeError:
        return [{"error": "NVD API å›å‚³é JSON æ ¼å¼"}]
    except Exception as e:
        return [{"error": f"NVD API éŒ¯èª¤: {e}"}]

    vulnerabilities = []
    for item in data.get("vulnerabilities", []):
        cve = item.get("cve", {})
        cve_id = cve.get("id", "")

        # å–å¾— CVSS åˆ†æ•¸
        cvss_score = 0.0
        cvss_vector = ""
        metrics = cve.get("metrics", {})
        if "cvssMetricV31" in metrics:
            cvss_data = metrics["cvssMetricV31"][0].get("cvssData", {})
            cvss_score = cvss_data.get("baseScore", 0.0)
            cvss_vector = cvss_data.get("vectorString", "")
        elif "cvssMetricV30" in metrics:
            cvss_data = metrics["cvssMetricV30"][0].get("cvssData", {})
            cvss_score = cvss_data.get("baseScore", 0.0)
            cvss_vector = cvss_data.get("vectorString", "")

        if cvss_score < min_cvss:
            continue

        # å–å¾—æè¿°
        descriptions = cve.get("descriptions", [])
        description = ""
        for desc in descriptions:
            if desc.get("lang") == "en":
                description = desc.get("value", "")
                break

        vulnerabilities.append({
            "cve_id": cve_id,
            "cvss": cvss_score,
            "cvss_vector": cvss_vector,
            "description": description[:500],
            "published": cve.get("published", ""),
            "url": f"https://nvd.nist.gov/vuln/detail/{cve_id}"
        })

    return vulnerabilities[:limit]


async def _fetch_cisa_kev(days: int, limit: int) -> list[dict]:
    """å¾ CISA KEV æŠ“å–å·²çŸ¥è¢«åˆ©ç”¨æ¼æ´"""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(
                "https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json"
            )
            response.raise_for_status()
            data = response.json()
    except httpx.TimeoutException:
        return [{"error": "CISA KEV è¶…æ™‚ (30s)"}]
    except httpx.HTTPStatusError as e:
        return [{"error": f"CISA KEV HTTP {e.response.status_code}"}]
    except httpx.RequestError as e:
        return [{"error": f"CISA KEV ç¶²è·¯éŒ¯èª¤: {type(e).__name__}"}]
    except json.JSONDecodeError:
        return [{"error": "CISA KEV å›å‚³é JSON æ ¼å¼"}]
    except Exception as e:
        return [{"error": f"CISA KEV API éŒ¯èª¤: {e}"}]

    cutoff_date = datetime.now() - timedelta(days=days)
    vulnerabilities = []

    for vuln in data.get("vulnerabilities", []):
        # è§£æåŠ å…¥æ—¥æœŸ
        date_added = vuln.get("dateAdded", "")
        try:
            added_date = datetime.strptime(date_added, "%Y-%m-%d")
            if added_date < cutoff_date:
                continue
        except ValueError:
            continue

        vulnerabilities.append({
            "cve_id": vuln.get("cveID", ""),
            "vendor": vuln.get("vendorProject", ""),
            "product": vuln.get("product", ""),
            "name": vuln.get("vulnerabilityName", ""),
            "description": vuln.get("shortDescription", ""),
            "date_added": date_added,
            "due_date": vuln.get("dueDate", ""),
            "in_kev": True,
            "url": f"https://nvd.nist.gov/vuln/detail/{vuln.get('cveID', '')}"
        })

        if len(vulnerabilities) >= limit:
            break

    return vulnerabilities


async def call_tool(name: str, arguments: dict[str, Any]) -> list[TextContent]:
    """åŸ·è¡Œæ–°èæ”¶é›†å·¥å…·"""

    if name == "list_news_sources":
        config = _load_sources_config()
        sources = config.get("sources", [])

        result = []
        for source in sources:
            item = {
                "name": source.get("name"),
                "type": source.get("type"),
                "category": source.get("category"),
                "priority": source.get("priority"),
                "language": source.get("language"),
            }
            # å¦‚æœæœ‰ status æˆ– noteï¼ŒåŠ å…¥é¡¯ç¤º
            if source.get("status"):
                item["status"] = source.get("status")
            if source.get("note"):
                item["note"] = source.get("note")
            result.append(item)

        return [TextContent(
            type="text",
            text=json.dumps(result, ensure_ascii=False, indent=2)
        )]

    elif name == "fetch_security_news":
        config = _load_sources_config()
        all_sources = config.get("sources", [])
        days = arguments.get("days", 7)
        limit = arguments.get("limit", 10)
        keywords = arguments.get("keywords")
        requested_sources = arguments.get("sources", [])

        # éæ¿¾ RSS é¡å‹çš„ä¾†æºï¼ˆæ’é™¤ disabled çš„ä¾†æºï¼‰
        rss_sources = [
            s for s in all_sources
            if s.get("type") == "rss" and s.get("status") != "disabled"
        ]

        # å¦‚æœæœ‰æŒ‡å®šä¾†æºï¼Œé€²è¡Œæ¯”å°
        if requested_sources:
            matched_sources = []
            for query in requested_sources:
                matched_sources.extend(_match_source(query, rss_sources))
            rss_sources = matched_sources

        if not rss_sources:
            return [TextContent(type="text", text="æ‰¾ä¸åˆ°ç¬¦åˆçš„ RSS ä¾†æº")]

        # ä¸¦è¡ŒæŠ“å–æ‰€æœ‰ä¾†æºçš„æ–°èï¼ˆå¤§å¹…æå‡æ•ˆèƒ½ï¼‰
        import asyncio

        async def fetch_source(source: dict) -> tuple[str, list[dict]]:
            source_name = source.get("name", "Unknown")
            url = source.get("url", "")
            if not url:
                return source_name, []
            articles = await _fetch_rss(url, days, limit, keywords)
            return source_name, articles

        # ä½¿ç”¨ asyncio.gather ä¸¦è¡ŒæŠ“å–
        tasks = [fetch_source(s) for s in rss_sources]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        all_articles = {}
        failed_sources = []
        for i, result in enumerate(results):
            source_name = rss_sources[i].get("name", f"ä¾†æº {i+1}")
            if isinstance(result, Exception):
                failed_sources.append({
                    "source": source_name,
                    "error": f"{type(result).__name__}: {result}"
                })
                continue
            name, articles = result
            all_articles[name] = articles

        # åœ¨çµæœä¸­åŠ å…¥çµ±è¨ˆå’Œå¤±æ•—è³‡è¨Š
        response = {
            "_meta": {
                "total_sources": len(rss_sources),
                "success": len(all_articles),
                "failed": len(failed_sources)
            }
        }
        response.update(all_articles)
        if failed_sources:
            response["_failed"] = failed_sources

        return [TextContent(
            type="text",
            text=json.dumps(response, ensure_ascii=False, indent=2)
        )]

    elif name == "fetch_vulnerabilities":
        min_cvss = arguments.get("min_cvss", 7.0)
        days = arguments.get("days", 7)
        include_kev = arguments.get("include_kev", True)
        limit = arguments.get("limit", 20)

        result = {
            "nvd": [],
            "kev": []
        }

        # å¾ NVD æŠ“å–
        result["nvd"] = await _fetch_nvd(min_cvss, days, limit)

        # å¾ CISA KEV æŠ“å–
        if include_kev:
            result["kev"] = await _fetch_cisa_kev(days, limit)

        # åˆä½µä¸¦æ¨™è¨˜ KEV ç‹€æ…‹
        kev_cves = {v["cve_id"] for v in result["kev"] if "cve_id" in v}
        for vuln in result["nvd"]:
            vuln["in_kev"] = vuln["cve_id"] in kev_cves

        return [TextContent(
            type="text",
            text=json.dumps(result, ensure_ascii=False, indent=2)
        )]

    elif name == "suggest_searches":
        category = arguments.get("category", "all")
        context = arguments.get("context", {})
        include_fetch_targets = arguments.get("include_fetch_targets", True)
        period_start = arguments.get("period_start")
        period_end = arguments.get("period_end")

        # è¼‰å…¥æœå°‹æ¨¡æ¿
        search_templates = _load_search_templates()
        if not search_templates:
            return [TextContent(type="text", text="æ‰¾ä¸åˆ°æœå°‹æ¨¡æ¿é…ç½®æª”")]

        # è§£ææ™‚é–“ç¯„åœ
        now = datetime.now()
        if period_start:
            try:
                start_date = datetime.strptime(period_start, "%Y-%m-%d")
            except ValueError:
                start_date = now - timedelta(days=7)
        else:
            start_date = now - timedelta(days=7)

        if period_end:
            try:
                end_date = datetime.strptime(period_end, "%Y-%m-%d")
            except ValueError:
                end_date = now
        else:
            end_date = now

        # æº–å‚™å‹•æ…‹è®Šæ•¸
        variables = {
            "year": str(start_date.year),
            "month": start_date.strftime("%B"),
            "month_zh": _month_to_chinese(start_date.month),
            "start_date": start_date.strftime("%Y-%m-%d"),
            "end_date": end_date.strftime("%Y-%m-%d"),
            "date_range": f"{start_date.strftime('%Y/%m/%d')}~{end_date.strftime('%Y/%m/%d')}",
            **context
        }

        # åˆ¤æ–·æ˜¯å¦ç‚ºæ­·å²æœå°‹
        is_historical = (now - end_date).days > 7

        result = {
            "web_searches": [],
            "fetch_targets": [],
            "period": {
                "start": start_date.strftime("%Y-%m-%d"),
                "end": end_date.strftime("%Y-%m-%d"),
                "is_historical": is_historical
            }
        }

        # æ”¶é›†æœå°‹å»ºè­°
        categories_to_process = (
            [category] if category != "all"
            else ["taiwan_news", "vulnerabilities", "threat_intel", "industry_specific"]
        )

        for cat in categories_to_process:
            cat_data = search_templates.get(cat, {})
            queries = cat_data.get("queries", [])
            for q in queries:
                query_template = q.get("query", "")
                # æ›¿æ›è®Šæ•¸
                try:
                    query = query_template.format(**variables)
                except KeyError:
                    # å¦‚æœæœ‰æœªæä¾›çš„è®Šæ•¸ï¼Œè·³éæ­¤æŸ¥è©¢
                    continue

                # å°æ–¼æ­·å²æœå°‹ï¼ŒåŠ å…¥æ™‚é–“ç¯„åœé™å®š
                if is_historical:
                    # åŠ å…¥ Google æ™‚é–“éæ¿¾èªæ³•
                    date_filter = f" after:{start_date.strftime('%Y-%m-%d')} before:{end_date.strftime('%Y-%m-%d')}"
                    query = query + date_filter

                result["web_searches"].append({
                    "query": query,
                    "priority": q.get("priority", "medium"),
                    "category": q.get("category", cat),
                    "note": q.get("note")
                })

        # å°æ–¼æ­·å²æœå°‹ï¼ŒåŠ å…¥é¡å¤–çš„æ™‚é–“é™å®šæœå°‹
        if is_historical:
            historical_queries = [
                {
                    "query": f"å°ç£ è³‡å®‰äº‹ä»¶ {start_date.strftime('%Yå¹´%mæœˆ')}",
                    "priority": "high",
                    "category": "news",
                    "note": "æ­·å²æ™‚é–“ç¯„åœæœå°‹"
                },
                {
                    "query": f"cybersecurity incident {start_date.strftime('%B %Y')}",
                    "priority": "high",
                    "category": "news",
                    "note": "æ­·å²æ™‚é–“ç¯„åœæœå°‹ (è‹±æ–‡)"
                },
                {
                    "query": f"CVE {start_date.strftime('%Y-%m')} critical",
                    "priority": "high",
                    "category": "vulnerability",
                    "note": "è©²æœˆä»½é‡å¤§æ¼æ´"
                },
            ]
            result["web_searches"].extend(historical_queries)

        # æ”¶é›† WebFetch ç›®æ¨™ï¼ˆæ­·å²æœå°‹æ™‚ä¸åŒ…å«ï¼Œå› ç‚ºç¶²é å…§å®¹æœƒæ˜¯æœ€æ–°çš„ï¼‰
        if include_fetch_targets and not is_historical:
            fetch_data = search_templates.get("fetch_targets", {})
            urls = fetch_data.get("urls", [])
            for target in urls:
                result["fetch_targets"].append({
                    "name": target.get("name"),
                    "url": target.get("url"),
                    "type": target.get("type"),
                    "priority": target.get("priority", "medium"),
                    "prompt": target.get("prompt")
                })
        elif is_historical:
            result["fetch_targets_note"] = "æ­·å²é€±å ±ä¸ä½¿ç”¨ WebFetchï¼Œå› ç‚ºç¶²é å…§å®¹æ˜¯æœ€æ–°çš„ã€‚è«‹ä¾è³´ WebSearch çµæœã€‚"

        # æŒ‰å„ªå…ˆç´šæ’åº
        priority_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
        result["web_searches"].sort(key=lambda x: priority_order.get(x["priority"], 99))
        if result["fetch_targets"]:
            result["fetch_targets"].sort(key=lambda x: priority_order.get(x["priority"], 99))

        return [TextContent(
            type="text",
            text=json.dumps(result, ensure_ascii=False, indent=2)
        )]

    elif name == "list_weekly_data":
        raw_dir = Path(__file__).parent.parent.parent.parent.parent.parent / "output" / "raw"

        if not raw_dir.exists():
            return [TextContent(type="text", text="å°šç„¡å·²ä¿å­˜çš„é€±å ±è³‡æ–™ã€‚GitHub Actions æœƒæ¯é€±ä¸€è‡ªå‹•æ”¶é›†ã€‚")]

        files = sorted(raw_dir.glob("*.json"), reverse=True)
        if not files:
            return [TextContent(type="text", text="å°šç„¡å·²ä¿å­˜çš„é€±å ±è³‡æ–™ã€‚")]

        result = {
            "available_weeks": [],
            "total_files": len(files),
            "storage_path": str(raw_dir)
        }

        for f in files:
            try:
                data = json.loads(f.read_text(encoding="utf-8"))
                meta = data.get("metadata", {})
                result["available_weeks"].append({
                    "week": meta.get("week", f.stem),
                    "filename": f.name,
                    "collected_at": meta.get("collected_at", "unknown"),
                    "stats": meta.get("stats", {})
                })
            except Exception:
                result["available_weeks"].append({
                    "week": f.stem,
                    "filename": f.name,
                    "error": "ç„¡æ³•è§£ææª”æ¡ˆ"
                })

        return [TextContent(
            type="text",
            text=json.dumps(result, ensure_ascii=False, indent=2)
        )]

    elif name == "load_weekly_data":
        raw_dir = Path(__file__).parent.parent.parent.parent.parent.parent / "output" / "raw"
        week = arguments.get("week")

        if not raw_dir.exists():
            return [TextContent(type="text", text="âŒ å°šç„¡å·²ä¿å­˜çš„é€±å ±è³‡æ–™ã€‚")]

        if week:
            # è¼‰å…¥æŒ‡å®šé€±æ•¸
            target_file = raw_dir / f"{week}.json"
        else:
            # è¼‰å…¥æœ€æ–°ä¸€é€±
            files = sorted(raw_dir.glob("*.json"), reverse=True)
            if not files:
                return [TextContent(type="text", text="âŒ å°šç„¡å·²ä¿å­˜çš„é€±å ±è³‡æ–™ã€‚")]
            target_file = files[0]

        if not target_file.exists():
            return [TextContent(type="text", text=f"âŒ æ‰¾ä¸åˆ°é€±å ±è³‡æ–™ï¼š{target_file.name}")]

        try:
            data = json.loads(target_file.read_text(encoding="utf-8"))
            meta = data.get("metadata", {})

            # å›å‚³æ‘˜è¦ + è³‡æ–™
            summary = f"""## ğŸ“Š é€±å ±åŸå§‹è³‡æ–™ï¼š{meta.get('week', target_file.stem)}

**æ”¶é›†æ™‚é–“**: {meta.get('collected_at', 'unknown')}
**è³‡æ–™æœŸé–“**: {meta.get('period', {}).get('start')} ~ {meta.get('period', {}).get('end')}

### çµ±è¨ˆ
- æ–°èæ–‡ç« : {meta.get('stats', {}).get('total_articles', 0)} å‰‡
- æ–°èä¾†æº: {meta.get('stats', {}).get('news_sources', 0)} å€‹
- NVD æ¼æ´: {meta.get('stats', {}).get('nvd_vulnerabilities', 0)} å€‹
- KEV æ¼æ´: {meta.get('stats', {}).get('kev_vulnerabilities', 0)} å€‹
- å»ºè­°æœå°‹: {meta.get('stats', {}).get('suggested_searches', 0)} å€‹

### ä½¿ç”¨æ–¹å¼
æ­¤è³‡æ–™å·²è¼‰å…¥ï¼Œä½ å¯ä»¥ï¼š
1. åˆ†ææ–°èè¶¨å‹¢ä¸¦æ’°å¯«æ‘˜è¦
2. åŸ·è¡Œå»ºè­°çš„ WebSearch æŸ¥è©¢è£œå……æœ€æ–°è³‡è¨Š
3. ä½¿ç”¨ `generate_report_draft` ç”¢ç”Ÿé€±å ±

---
### å®Œæ•´è³‡æ–™
"""
            return [TextContent(
                type="text",
                text=summary + json.dumps(data, ensure_ascii=False, indent=2)
            )]

        except Exception as e:
            return [TextContent(type="text", text=f"âŒ è¼‰å…¥è³‡æ–™å¤±æ•—ï¼š{e}")]


def _month_to_chinese(month: int) -> str:
    """å°‡æœˆä»½æ•¸å­—è½‰æ›ç‚ºä¸­æ–‡"""
    months = ["ä¸€æœˆ", "äºŒæœˆ", "ä¸‰æœˆ", "å››æœˆ", "äº”æœˆ", "å…­æœˆ",
              "ä¸ƒæœˆ", "å…«æœˆ", "ä¹æœˆ", "åæœˆ", "åä¸€æœˆ", "åäºŒæœˆ"]
    return months[month - 1] if 1 <= month <= 12 else str(month)
